<!DOCTYPE html>
<html lang="pl">
<head>
  <meta charset="UTF-8" />
  <title>Gadaczka</title>
</head>
<body>
  <h1>Gadaczka v1</h1>
  <p>Kliknij gdziekolwiek, aby aktywować mikrofon.</p>

 <script>
class VoiceAssistant {
  constructor(config = {}) {
    this.sampleDuration = config.sampleDuration || 3500;
    this.interruptThreshold = config.interruptThreshold || 25;
    this.fftSize = config.fftSize || 512;

    this.audio = new Audio();
    this.isSpeaking = false;
    this.isListening = false;

    this.stream = null;
    this.context = null;
    this.analyser = null;
    this.source = null;
    this.mediaRecorder = null;
    this.audioChunks = [];
  }

  async init() {
    await this.listenLoop();
  }

  async cleanupAudio() {
    if (this.mediaRecorder && this.mediaRecorder.state === "recording") {
      this.mediaRecorder.stop();
    }
    if (this.stream) {
      this.stream.getTracks().forEach(t => t.stop());
      this.stream = null;
    }
    if (this.context && this.context.state !== "closed") {
      await this.context.close();
      this.context = null;
    }
    this.isListening = false;
  }

  async listenLoop() {
    if (this.isSpeaking || this.isListening) return;
    this.isListening = true;

    await this.cleanupAudio();

    this.stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    this.context = new AudioContext();
    this.source = this.context.createMediaStreamSource(this.stream);
    this.analyser = this.context.createAnalyser();
    this.analyser.fftSize = this.fftSize;
    this.source.connect(this.analyser);

    this.audioChunks = [];
    this.mediaRecorder = new MediaRecorder(this.stream);
    this.mediaRecorder.ondataavailable = e => this.audioChunks.push(e.data);
    this.mediaRecorder.onstop = () => {
      this.isListening = false;
      this.sendToServer();
    };

    this.mediaRecorder.start();

    const dataArray = new Uint8Array(this.analyser.frequencyBinCount);
    let silenceStart = null;
    const silenceDelay = 500; // krótszy delay ciszy

    const checkSilence = () => {
      this.analyser.getByteFrequencyData(dataArray);
      const avg = dataArray.reduce((a, b) => a + b) / dataArray.length;

      const now = Date.now();

      if (avg < 20) {
        if (!silenceStart) silenceStart = now;
        if (now - silenceStart > silenceDelay && this.mediaRecorder.state === "recording") {
          console.log("Cisza wykryta – przerywam nagrywanie");
          this.mediaRecorder.stop();
          this.cleanupAudio(); // asynchronicznie, event onstop odpali sendToServer
          return;
        }
      } else {
        silenceStart = null;
      }

      if (this.mediaRecorder.state === "recording") {
        requestAnimationFrame(checkSilence);
      }
    };

    requestAnimationFrame(checkSilence);

    setTimeout(() => {
      if (this.mediaRecorder.state === "recording") {
        console.log("Maksymalny czas minął – przerywam nagrywanie");
        this.mediaRecorder.stop();
        this.cleanupAudio();
      }
    }, this.sampleDuration);
  }

  async sendToServer() {
    const blob = new Blob(this.audioChunks, { type: 'audio/webm' });
    const formData = new FormData();
    formData.append('audio', blob);

    const res = await fetch('/api/stt', { method: 'POST', body: formData });
    const { transcript } = await res.json();
    console.log("Użytkownik powiedział:", transcript);

    const replyRes = await fetch('/api/reply', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ userText: transcript }),
    });
    const { reply } = await replyRes.json();

    this.speak(reply);
  }

  async speak(text) {
    this.isSpeaking = true;

    // Wyłącz nasłuch na czas mowy
    await this.cleanupAudio();

    const res = await fetch('/api/tts', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ text }),
    });

    const blob = await res.blob();
    const url = URL.createObjectURL(blob);

    this.audio.src = url;

    return new Promise(resolve => {
      this.audio.onended = () => {
        this.isSpeaking = false;
        this.listenLoop(); // wznow nasłuch
        resolve();
      };
      this.audio.play();

      // Nasłuch mowy użytkownika podczas odtwarzania
      this.monitorUserInterrupt();
    });
  }

  async monitorUserInterrupt() {
    if (this.isListening) return; // nie monitoruj jeśli już nasłuchuje

    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    const context = new AudioContext();
    const source = context.createMediaStreamSource(stream);
    const analyser = context.createAnalyser();
    analyser.fftSize = this.fftSize;
    source.connect(analyser);
    const dataArray = new Uint8Array(analyser.frequencyBinCount);

    const checkVolume = () => {
      analyser.getByteFrequencyData(dataArray);
      const avg = dataArray.reduce((a, b) => a + b) / dataArray.length;

      // DEBUG: console.log("Volume avg:", avg.toFixed(2));

      if (avg > this.interruptThreshold) {
        console.log("Wykryto mowę użytkownika — przerywam!");
        this.audio.pause();
        this.audio.currentTime = 0;
        stream.getTracks().forEach(track => track.stop());
        context.close();
        this.isSpeaking = false;
        this.listenLoop();
      } else if (!this.audio.paused) {
        requestAnimationFrame(checkVolume);
      } else {
        stream.getTracks().forEach(track => track.stop());
        context.close();
        this.isSpeaking = false;
        this.listenLoop();
      }
    };

    checkVolume();
  }
}

// ✅ Ustawienia
const assistant = new VoiceAssistant({
  sampleDuration: 2500,      // krótszy czas nagrywania — szybciej kończy
  interruptThreshold: 40,    // większy próg, mniej fałszywych startów mowy
  fftSize: 512
});

document.addEventListener('click', () => {
  assistant.init();
}, { once: true });

</script>

</body>
</html>
